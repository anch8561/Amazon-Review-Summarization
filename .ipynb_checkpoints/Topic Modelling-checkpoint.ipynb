{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF,  LatentDirichletAllocation\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "import gzip\n",
    "import simplejson as json\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import os\\nimport gzip\\nimport json\\n\\ndef parse(path):\\n    g = gzip.open(path, 'r')\\n    for l in g:\\n        yield json.loads(l)\\n\\nfileName = 'Electronics_5'\\ninputFileName  = fileName + '.json.gz'\\nif not os.path.exists(fileName): os.makedirs(fileName)\\n\\ncount = 0\\nii = 0\\ndata = {}\\nasin = '0'\\nfor review in parse(inputFileName):\\n    if review['asin'] != asin:\\n        outputFileName = fileName + r'/' + asin + '.json'\\n        outputFile = open(outputFileName, 'w', newline='')\\n        json.dump(data, outputFile)\\n        outputFile.close\\n        ii = 0\\n        data = {}\\n        asin = review['asin']\\n    try: helpfulness = review['vote']\\n    except: helpfulness = '0'\\n    try:\\n        data[ii] = {\\n            'helpfulness': helpfulness,\\n            'rating': review['overall'],\\n            'text': review['reviewText']}\\n        ii += 1\\n    except: pass\\n    count += 1\\n    if count % 1e5 == 0: print(count)\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Script for SAVING DATA. Uncomment it when needed\n",
    "\n",
    "\"\"\"import os\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "fileName = 'Electronics_5'\n",
    "inputFileName  = fileName + '.json.gz'\n",
    "if not os.path.exists(fileName): os.makedirs(fileName)\n",
    "\n",
    "count = 0\n",
    "ii = 0\n",
    "data = {}\n",
    "asin = '0'\n",
    "for review in parse(inputFileName):\n",
    "    if review['asin'] != asin:\n",
    "        outputFileName = fileName + r'/' + asin + '.json'\n",
    "        outputFile = open(outputFileName, 'w', newline='')\n",
    "        json.dump(data, outputFile)\n",
    "        outputFile.close\n",
    "        ii = 0\n",
    "        data = {}\n",
    "        asin = review['asin']\n",
    "    try: helpfulness = review['vote']\n",
    "    except: helpfulness = '0'\n",
    "    try:\n",
    "        data[ii] = {\n",
    "            'helpfulness': helpfulness,\n",
    "            'rating': review['overall'],\n",
    "            'text': review['reviewText']}\n",
    "        ii += 1\n",
    "    except: pass\n",
    "    count += 1\n",
    "    if count % 1e5 == 0: print(count)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "   \n",
    "    def transform(self, X, **transform_params):\n",
    "        #return [cleanText(text) for text in X]\n",
    "        return [text for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "\"\"\"def cleanText(text):\n",
    "    \"this function removes new lines.\"\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    return text\n",
    "\"\"\"\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    \"This function tokenizes text and does other preprocessing steps like Lemmatization and Stemming.\"\n",
    "\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #tokenize\n",
    "    tokens = tokenizer.tokenize(sample)\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for word in tokens:\n",
    "        if word.isalnum() and not word in stop_words:\n",
    "            word = word.lower()\n",
    "            word = lemmatizer.lemmatize(word, pos = 'v')\n",
    "            lemmas.append(word)\n",
    "    tokens = lemmas\n",
    "    # white space removal and new line removal\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "def return_topics(vectorizer, clf, W, df, n_top_words, n_top_documents):\n",
    "    print('return topics')\n",
    "    topics, reviews = [], []\n",
    "    features = vectorizer.get_feature_names()\n",
    "    sentiment_analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for topic_id, topic in enumerate(clf.components_):\n",
    "\n",
    "        # grab the list of words describing the topic\n",
    "        topic_word_list = []\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            topic_word_list.append(features[i])\n",
    "\n",
    "        # split words in case there are some bigrams\n",
    "        split_topic_word_list = []\n",
    "        for word in topic_word_list:\n",
    "            for splitted in word.split():\n",
    "                split_topic_word_list.append(splitted)\n",
    "        topic_words = list(set(split_topic_word_list))\n",
    "\n",
    "        # append topic words as a single string\n",
    "        topics.append(' '.join([word for word in topic_words]))\n",
    "\n",
    "        # iterate for reviews for each topic\n",
    "        topic_doc_indices = np.argsort(W[:, topic_id])[::-1][0:n_top_documents]\n",
    "\n",
    "        for doc_ind in topic_doc_indices:\n",
    "            review = df['reviewText'].iloc[doc_ind]\n",
    "\n",
    "            # check if the review contains any of the topic words\n",
    "            if any(word in review.lower() for word in topic_words):\n",
    "                # analyse sentiment\n",
    "                vader = sentiment_analyser.polarity_scores(review)\n",
    "                # form the review - topic_id and sentiment data structure\n",
    "                reviews.append(df.iloc[doc_ind].to_dict())\n",
    "                reviews[-1]['topic'] = topic_id\n",
    "                reviews[-1]['sentiment'] = vader['compound']\n",
    "\n",
    "    return topics, reviews\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def summarize_reviews(topics, reviews):\n",
    "    # topics: list of strings. Each string contains the topics for a review\n",
    "    # reviews: list of dicts with the following fields\n",
    "    #  'reviewText': string with text of the review\n",
    "    #  'topic': topics index\n",
    "    # returns reviews with the following new fields\n",
    "    #  'summary': sentences from review w/ topic words\n",
    "\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    summary_all_review = []\n",
    "    for ii, review in enumerate(reviews):\n",
    "        summary = []\n",
    "        sentences = sent_tokenize(review['reviewText'])\n",
    "        topic_words = topics[review['topic']].split()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for word in topic_words:\n",
    "                if word in sentence.lower():\n",
    "                    summary.append(sentence)\n",
    "                    break\n",
    "\n",
    "        reviews[ii]['summary'] = ' '.join([sentence for sentence in summary])\n",
    "        vader = analyser.polarity_scores(reviews[ii]['summary'])\n",
    "        reviews[ii]['summary_sentiment'] = vader['compound']\n",
    "        \n",
    "        summary_all_review.append(reviews[ii]['summary'])\n",
    "\n",
    "    return reviews, summary_all_review\n",
    "\n",
    "def print_topics(test_asin):\n",
    "\n",
    "    test_df = reviews_df[reviews_df['asin'] == test_asin].dropna()\n",
    "    n_features, n_top_words, n_topics, n_top_documents = 1000, 3, 8, 3\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=n_features,\n",
    "                                 tokenizer=tokenizeText,\n",
    "                                 stop_words='english',\n",
    "                                 ngram_range=(1,2))\n",
    "\n",
    "    clf = NMF(n_components=n_topics, random_state=1, solver='mu', beta_loss='frobenius')\n",
    "   \n",
    "    #clf = LatentDirichletAllocation(n_components = 5, max_iter = 5, learning_method ='online',learning_offset = 50.,random_state = 0)\n",
    "\n",
    "    pipe = Pipeline([('cleanText', CleanTextTransformer()),('vectorizer', vectorizer), ('nmf', clf)])\n",
    "\n",
    "    pipe.fit(test_df['reviewText'])\n",
    "    transform = pipe.fit_transform(test_df['reviewText'])\n",
    "    \n",
    "    #topic identification\n",
    "    topics, reviews = return_topics(vectorizer, clf, transform, test_df, n_top_words, n_top_documents)\n",
    "    # review summarization\n",
    "    reviews , summary = summarize_reviews(topics, reviews)\n",
    "    print(\"Summary :\\n\", len(summary))\n",
    "    print(\"Topics:\", len(topics))\n",
    "    \n",
    "    return topics, reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin              reviewerName   helpful  \\\n",
      "0   AO94DHGC771SJ  0528881469                   amazdnu    [0, 0]   \n",
      "1   AMO214LNFCEI4  0528881469           Amazon Customer  [12, 15]   \n",
      "2  A3N7T0DY83Y4IG  0528881469             C. A. Freeman  [43, 45]   \n",
      "3  A1H8PY3QHMQQA0  0528881469  Dave M. Shaw \"mack dave\"   [9, 10]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  We got this GPS for my husband who is an (OTR)...      5.0   \n",
      "1  I'm a professional OTR truck driver, and I bou...      1.0   \n",
      "2  Well, what can I say.  I've had this unit in m...      3.0   \n",
      "3  Not going to write a long review, even thought...      2.0   \n",
      "\n",
      "                   summary  unixReviewTime   reviewTime  \n",
      "0          Gotta have GPS!      1370131200   06 2, 2013  \n",
      "1        Very Disappointed      1290643200  11 25, 2010  \n",
      "2           1st impression      1283990400   09 9, 2010  \n",
      "3  Great grafics, POOR GPS      1290556800  11 24, 2010  \n"
     ]
    }
   ],
   "source": [
    "#reviews_df = getDF('Video_Games_5.json.gz')\n",
    "reviews_df = getDF('Electronics_5.json.gz')\n",
    "print(reviews_df.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy(n):\n",
    "    return -np.log(1/n)\n",
    "\n",
    "def unique(sequence):\n",
    "    '''get unique elements of list and keep the same order'''\n",
    "    \n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "def redundancy(string):\n",
    "    entropy, string_list = 0, string.split()\n",
    "    string_set = unique(string_list)\n",
    "    for word in string_set:\n",
    "        p = string_list.count(word)/len(string_list)\n",
    "        entropy -= p*np.log(p)        \n",
    "    return 1 - entropy/max_entropy(len(string_list))\n",
    "\n",
    "def lemmatize(string):\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = parser(string)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(stemmer.stem(token.lemma_.lower().strip()))\n",
    "        \n",
    "    return ' '.join(lemma for lemma in lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certify midrange bang buck live room home theater altec lansing\n",
      "0.019223019952826492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return topics\n",
      "Summary :\n",
      " 24\n",
      "Topics: 8\n",
      "['sound bass music', 'z 2300', 'sound great', 'buy computer speakers', 'satellite cable remote', 'sound card good', 'bose work great', 'set best speaker']\n",
      "sound bass music z 2300 sound great buy computer speakers satellite cable remote sound card good bose work great set best speaker\n",
      "0.06885190025657495\n"
     ]
    }
   ],
   "source": [
    "string = 'speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certified midrange bang for the buck living room home theater altec lansing'\n",
    "lemmas = tokenizeText(string) \n",
    "lemmas = ' '.join(lemma for lemma in lemmas)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "\n",
    "topics, reviews = print_topics('B0002SQ2P2')\n",
    "\n",
    "print(topics)\n",
    "# print(emoji_topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòê - work use mouse\n",
      "üôÇ - wheel trackman marble\n",
      "üôÇ - wireless version wire\n",
      "üòê - ball track\n",
      "üôÇ - trackball button easy\n",
      "üòê - thumb pad mouse\n",
      "üòê - 5 years logitech\n",
      "üôÇ - finger button thumb\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "for emoji_topic in emoji_topics:\n",
    "    print(emoji.emojize(emoji_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earbuds sound quality couple months every 6 months sound good great sound bass volume ears earbud earphones hear inexpensive jack model break cord\n",
      "0.06492466861659307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return topics\n",
      "Summary :\n",
      " 24\n",
      "Topics: 8\n",
      "['sound music headphones', 'home years studio', 'ears head wear', 'comfortable great', 'response hz flat', 'expect price feel', 'say read review', 'amp headphone fit']\n",
      "sound music headphones home years studio ears head wear comfortable great response hz flat expect price feel say read review amp headphone fit\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "string = 'earbuds sound quality couple months every 6 months sound was good great sound bass volume ears earbud earphones hear inexpensive jack model broke cord'\n",
    "lemmas = tokenizeText(string) \n",
    "lemmas = ' '.join(lemma for lemma in lemmas)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "\n",
    "\n",
    "\n",
    "topics, review = print_topics('B0002D03ZW')\n",
    "print(topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_topic_model(test_asin):\n",
    "    search_params = {'n_components': [3,4,5,6,7,8,9,10,11,12], \"solver\": ['cd','mu']}\n",
    "    test_df = reviews_df[reviews_df['asin'] == test_asin].dropna()\n",
    "    n_features, n_top_words, n_topics, n_top_documents = 1000, 3, 6, 3\n",
    "    vectorizer = TfidfVectorizer(max_features=n_features,\n",
    "                                 tokenizer=tokenizeText,\n",
    "                                 stop_words='english',\n",
    "                                 ngram_range=(1,2))\n",
    "\n",
    "    clf = NMF()\n",
    "\n",
    "    pipe = Pipeline([('cleanText', CleanTextTransformer()),('vectorizer', vectorizer)])\n",
    "\n",
    "    # pipe.fit(test_df['reviewText'])\n",
    "    data_vectorized = pipe.fit_transform(test_df['reviewText'])\n",
    "\n",
    "    model = GridSearchCV(clf, param_grid=search_params)\n",
    "\n",
    "    model.fit(data_vectorized)\n",
    "    best_topic_model = model.best_estimator_\n",
    "    print(\"Best Model's Params: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n    n_components=None, random_state=None, shuffle=False, solver='cd',\n    tol=0.0001, verbose=0) does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-ece43947f05a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid_search_topic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'B0002SQ2P2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-d9ae001b0e02>\u001b[0m in \u001b[0;36mgrid_search_topic_model\u001b[1;34m(test_asin)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msearch_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_vectorized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mbest_topic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best Model's Params: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pycharmprojects\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         scorers, self.multimetric_ = _check_multimetric_scoring(\n\u001b[1;32m--> 629\u001b[1;33m             self.estimator, scoring=self.scoring)\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultimetric_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pycharmprojects\\venv\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[1;34m(estimator, scoring)\u001b[0m\n\u001b[0;32m    471\u001b[0m     if callable(scoring) or scoring is None or isinstance(scoring,\n\u001b[0;32m    472\u001b[0m                                                           str):\n\u001b[1;32m--> 473\u001b[1;33m         \u001b[0mscorers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pycharmprojects\\venv\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[1;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[0;32m    424\u001b[0m                 \u001b[1;34m\"If no scoring is specified, the estimator passed should \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;34m\"have a 'score' method. The estimator %r does not.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                 % estimator)\n\u001b[0m\u001b[0;32m    427\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         raise ValueError(\"For evaluating multiple scores, use \"\n",
      "\u001b[1;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n    n_components=None, random_state=None, shuffle=False, solver='cd',\n    tol=0.0001, verbose=0) does not."
     ]
    }
   ],
   "source": [
    "grid_search_topic_model('B0002SQ2P2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA Best Model's Params:  {'learning_decay': 0.5, 'n_components': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_lda(test_asin):\n",
    "\n",
    "    test_df = reviews_df[reviews_df['asin'] == test_asin].dropna()\n",
    "    n_features, n_top_words, n_topics, n_top_documents = 1000, 3, 8, 3\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=n_features,\n",
    "                                 tokenizer=tokenizeText,\n",
    "                                 stop_words='english',\n",
    "                                 ngram_range=(1,2))\n",
    "\n",
    "    clf = NMF(n_components=n_topics, random_state=1, solver='mu', beta_loss='frobenius')\n",
    "   \n",
    "    clf = LatentDirichletAllocation(n_components = 3, max_iter = 5, learning_method ='online',learning_offset = 50.,random_state = 0)\n",
    "\n",
    "    pipe = Pipeline([('cleanText', CleanTextTransformer()),('vectorizer', vectorizer), ('nmf', clf)])\n",
    "\n",
    "    pipe.fit(test_df['reviewText'])\n",
    "    transform = pipe.fit_transform(test_df['reviewText'])\n",
    "    \n",
    "    #topic identification\n",
    "    topics, reviews = return_topics(vectorizer, clf, transform, test_df, n_top_words, n_top_documents)\n",
    "    # review summarization\n",
    "    summary = summarize_reviews(topics, reviews)\n",
    "    #print(\"Summary :\\n\", summary)\n",
    "    print(\"Topics:\")\n",
    "    \n",
    "    return topics, reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certify midrange bang buck live room home theater altec lansing\n",
      "0.019223019952826492\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_topics_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-e3debb8d9f9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprint_topics_lda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'B0002SQ2P2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_topics_lda' is not defined"
     ]
    }
   ],
   "source": [
    "string = 'speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certified midrange bang for the buck living room home theater altec lansing'\n",
    "lemmas = tokenizeText(string) \n",
    "lemmas = ' '.join(lemma for lemma in lemmas)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "\n",
    "\n",
    "\n",
    "topics, review = print_topics_lda('B0002SQ2P2')\n",
    "print(topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amazon_asins = ['B0002SQ2P2', 'B0002KVQBA', 'B00029MTMQ','B00020S7XK','B00063E2HS','B0002D03ZW','B0002WPSBC','B00006JN3G','B0002CZHN6','B00004T8R2','B00004ZCJJ',\n",
    "'B00007M1TZ','B0002UPGOI','B000204SWE','B0002EMY9Y','B00006IAKJ','B000629GES','B00017LSPI','B0002UM0JW',\n",
    "'B0000C3GWU','B0001NNLHK','B0000BYDKO','B00008MOPJ','B00066HP7Y','B0000AQIFW','B00066EK2W','B00005NIMJ',\n",
    "'B00009WQS1','B0000DJEK7','B00028D778','B00030CHRQ','B0002IOIMQ','B0001EMA80','B00006JILE','B0002Y5WXO',\n",
    "'B00062UW5A','B00007GQLU','B00004TS16','B00005QXWI','B00018MSNI']\n",
    "\n",
    "test_elecronics5_asins = reviews_df['asin']\n",
    "print(list(test_elecronics5_asins[:5]))\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "print(intersection(test_amazon_asins, test_elecronics5_asins)) \n",
    "\n",
    "valid_asins = intersection(test_amazon_asins, test_elecronics5_asins)\n",
    "\n",
    "print(valid_asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_nmf = 0 \n",
    "\n",
    "redundancy_arr = ()\n",
    "\n",
    "for asin in valid_asins:\n",
    "    topics, review = print_topics(asin)\n",
    "    print(asin,topics)\n",
    "    lemmas = ' '.join(lemma for lemma in topics)\n",
    "    temp_red = redundancy(lemmas)\n",
    "    red_nmf += temp_red\n",
    "    redundancy_arr = list(redundancy_arr)\n",
    "    redundancy_arr.append((asin,temp_red))\n",
    "    redundancy_arr = tuple(redundancy_arr)\n",
    "    \n",
    "red_lda = 0 \n",
    "\n",
    "for asin in valid_asins:\n",
    "    topics, review = print_topics_lda(asin)\n",
    "    print(asin, topics)\n",
    "    lemmas = ' '.join(lemma for lemma in topics)\n",
    "    red_lda += redundancy(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "print(redundancy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asins_count = len(valid_asins)\n",
    "\n",
    "avd_red_nmf = red_nmf/asins_count\n",
    "avg_red_lda = red_lda/asins_count\n",
    "\n",
    "print('Avg reduncdancy NMF:', avd_red_nmf)\n",
    "print('Avg reduncdancy LDA:', avg_red_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_tags_df = pd.read_csv('amazon_scraped_tags.csv',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(scraped_tags_df.columns)\n",
    "\n",
    "tags_df = scraped_tags_df.iloc[: , 2 : 20]\n",
    "tags_df.head(5)\n",
    "\n",
    "tags_df['CombinedTags'] = tags_df[tags_df.columns[1:]].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "asin_tags_df = tags_df.loc[ : , ['ASIN','CombinedTags'] ]\n",
    "asin_tags_df= asin_tags_df[asin_tags_df.ASIN.isin(valid_asins)]\n",
    "asin_tags_df.to_csv('valid_asin_scraped_tags_df.csv')\n",
    "\n",
    "asin_tags_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_asins = list(asin_tags_df['ASIN'])\n",
    "print(valid_asins)\n",
    "review_sentences = list(asin_tags_df['CombinedTags'])\n",
    "redundancy_arr = ()\n",
    "\n",
    "red_amazon_scraped_tags = 0 \n",
    "for i in range(len(review_sentences)):\n",
    "    temp_red = redundancy(review_sentences[i])\n",
    "    red_amazon_scraped_tags += temp_red\n",
    "    redundancy_arr = list(redundancy_arr)\n",
    "    redundancy_arr.append((valid_asins[i],temp_red))\n",
    "    redundancy_arr = tuple(redundancy_arr)\n",
    "    \n",
    "asins_count = len(valid_asins)\n",
    "\n",
    "avd_red_amazon_scraped_tags = red_amazon_scraped_tags/asins_count\n",
    "\n",
    "print('Avg reduncdancy NMF:', avd_red_amazon_scraped_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asins_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(redundancy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return topics\n",
      "Summary :\n",
      " 23\n",
      "Topics: 8\n",
      "üôÇ - bass listen music\n",
      "üôÇ - good fit supply\n",
      "üôÇ - cord best headphones\n",
      "üôÇ - sound block noise\n",
      "üôÇ - bud hear really\n",
      "üôÇ - seal ear phone\n",
      "üòê - earphones price star\n",
      "üôÇ - plastic cover wire\n"
     ]
    }
   ],
   "source": [
    "#add emojis to topics\n",
    "import emoji\n",
    "\n",
    "topics, review = print_topics('B0001NNLHK')\n",
    "\n",
    "emoji_topics = []\n",
    "for topic_id, topic in enumerate(topics):\n",
    "\n",
    "    # grab the average sentiment\n",
    "    reviews_df = pd.DataFrame(reviews)\n",
    "    reviews_df = reviews_df[reviews_df['topic'] == topic_id]\n",
    "    polarity   = reviews_df['summary_sentiment'].mean()\n",
    "\n",
    "    # append emojis to topic name based on range of sentiment\n",
    "    if polarity <= -0.5:\n",
    "        emoji_topics.append(\"üòï - \"+topic)\n",
    "    elif polarity > -0.5 and polarity < 0.5:\n",
    "        emoji_topics.append(\"üòê - \"+topic)\n",
    "    else:\n",
    "        emoji_topics.append(\"üôÇ - \"+topic)\n",
    "\n",
    "for emoji_topic in emoji_topics:\n",
    "    print(emoji.emojize(emoji_topic))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
