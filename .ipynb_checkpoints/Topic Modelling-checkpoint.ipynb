{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF,  LatentDirichletAllocation\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "import gzip\n",
    "import simplejson as json\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import os\\nimport gzip\\nimport json\\n\\ndef parse(path):\\n    g = gzip.open(path, 'r')\\n    for l in g:\\n        yield json.loads(l)\\n\\nfileName = 'Electronics_5'\\ninputFileName  = fileName + '.json.gz'\\nif not os.path.exists(fileName): os.makedirs(fileName)\\n\\ncount = 0\\nii = 0\\ndata = {}\\nasin = '0'\\nfor review in parse(inputFileName):\\n    if review['asin'] != asin:\\n        outputFileName = fileName + r'/' + asin + '.json'\\n        outputFile = open(outputFileName, 'w', newline='')\\n        json.dump(data, outputFile)\\n        outputFile.close\\n        ii = 0\\n        data = {}\\n        asin = review['asin']\\n    try: helpfulness = review['vote']\\n    except: helpfulness = '0'\\n    try:\\n        data[ii] = {\\n            'helpfulness': helpfulness,\\n            'rating': review['overall'],\\n            'text': review['reviewText']}\\n        ii += 1\\n    except: pass\\n    count += 1\\n    if count % 1e5 == 0: print(count)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Script for SAVING DATA. Uncomment it when needed\n",
    "\n",
    "\"\"\"import os\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "fileName = 'Electronics_5'\n",
    "inputFileName  = fileName + '.json.gz'\n",
    "if not os.path.exists(fileName): os.makedirs(fileName)\n",
    "\n",
    "count = 0\n",
    "ii = 0\n",
    "data = {}\n",
    "asin = '0'\n",
    "for review in parse(inputFileName):\n",
    "    if review['asin'] != asin:\n",
    "        outputFileName = fileName + r'/' + asin + '.json'\n",
    "        outputFile = open(outputFileName, 'w', newline='')\n",
    "        json.dump(data, outputFile)\n",
    "        outputFile.close\n",
    "        ii = 0\n",
    "        data = {}\n",
    "        asin = review['asin']\n",
    "    try: helpfulness = review['vote']\n",
    "    except: helpfulness = '0'\n",
    "    try:\n",
    "        data[ii] = {\n",
    "            'helpfulness': helpfulness,\n",
    "            'rating': review['overall'],\n",
    "            'text': review['reviewText']}\n",
    "        ii += 1\n",
    "    except: pass\n",
    "    count += 1\n",
    "    if count % 1e5 == 0: print(count)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "   \n",
    "    def transform(self, X, **transform_params):\n",
    "        #return [cleanText(text) for text in X]\n",
    "        return [text for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "\"\"\"def cleanText(text):\n",
    "    \"this function removes new lines.\"\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    return text\n",
    "\"\"\"\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    \"This function tokenizes text and does other preprocessing steps like Lemmatization and Stemming.\"\n",
    "\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #tokenize\n",
    "    tokens = tokenizer.tokenize(sample)\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for word in tokens:\n",
    "        if word.isalnum() and not word in stop_words:\n",
    "            word = word.lower()\n",
    "            word = lemmatizer.lemmatize(word, pos = 'v')\n",
    "            lemmas.append(word)\n",
    "    tokens = lemmas\n",
    "    # white space removal and new line removal\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "def return_topics(vectorizer, clf, W, df, n_top_words, n_top_documents):\n",
    "    print('return topics')\n",
    "    topics, reviews = [], []\n",
    "    features = vectorizer.get_feature_names()\n",
    "    sentiment_analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for topic_id, topic in enumerate(clf.components_):\n",
    "\n",
    "        # grab the list of words describing the topic\n",
    "        topic_word_list = []\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            topic_word_list.append(features[i])\n",
    "\n",
    "        # split words in case there are some bigrams\n",
    "        split_topic_word_list = []\n",
    "        for word in topic_word_list:\n",
    "            for splitted in word.split():\n",
    "                split_topic_word_list.append(splitted)\n",
    "        topic_words = list(set(split_topic_word_list))\n",
    "\n",
    "        # append topic words as a single string\n",
    "        topics.append(' '.join([word for word in topic_words]))\n",
    "\n",
    "        # iterate for reviews for each topic\n",
    "        topic_doc_indices = np.argsort(W[:, topic_id])[::-1][0:n_top_documents]\n",
    "\n",
    "        for doc_ind in topic_doc_indices:\n",
    "            review = df['reviewText'].iloc[doc_ind]\n",
    "\n",
    "            # check if the review contains any of the topic words\n",
    "            if any(word in review.lower() for word in topic_words):\n",
    "                # analyse sentiment\n",
    "                vader = sentiment_analyser.polarity_scores(review)\n",
    "                # form the review - topic_id and sentiment data structure\n",
    "                reviews.append(df.iloc[doc_ind].to_dict())\n",
    "                reviews[-1]['topic'] = topic_id\n",
    "                reviews[-1]['sentiment'] = vader['compound']\n",
    "\n",
    "    return topics, reviews\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def summarize_reviews(topics, reviews):\n",
    "    # topics: list of strings. Each string contains the topics for a review\n",
    "    # reviews: list of dicts with the following fields\n",
    "    #  'reviewText': string with text of the review\n",
    "    #  'topic': topics index\n",
    "    # returns reviews with the following new fields\n",
    "    #  'summary': sentences from review w/ topic words\n",
    "\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    summary_all_review = []\n",
    "    for ii, review in enumerate(reviews):\n",
    "        summary = []\n",
    "        sentences = sent_tokenize(review['reviewText'])\n",
    "        topic_words = topics[review['topic']].split()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for word in topic_words:\n",
    "                if word in sentence.lower():\n",
    "                    summary.append(sentence)\n",
    "                    break\n",
    "\n",
    "        reviews[ii]['summary'] = ' '.join([sentence for sentence in summary])\n",
    "        vader = analyser.polarity_scores(reviews[ii]['summary'])\n",
    "        reviews[ii]['summary_sentiment'] = vader['compound']\n",
    "        \n",
    "        summary_all_review.append(reviews[ii]['summary'])\n",
    "\n",
    "    return summary_all_review\n",
    "\n",
    "def print_topics(test_asin):\n",
    "\n",
    "    test_df = reviews_df[reviews_df['asin'] == test_asin].dropna()\n",
    "    n_features, n_top_words, n_topics, n_top_documents = 1000, 3, 8, 3\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=n_features,\n",
    "                                 tokenizer=tokenizeText,\n",
    "                                 stop_words='english',\n",
    "                                 ngram_range=(1,2))\n",
    "\n",
    "    clf = NMF(n_components=n_topics, random_state=1, solver='mu', beta_loss='frobenius')\n",
    "   \n",
    "    #clf = LatentDirichletAllocation(n_components = 5, max_iter = 5, learning_method ='online',learning_offset = 50.,random_state = 0)\n",
    "\n",
    "    pipe = Pipeline([('cleanText', CleanTextTransformer()),('vectorizer', vectorizer), ('nmf', clf)])\n",
    "\n",
    "    pipe.fit(test_df['reviewText'])\n",
    "    transform = pipe.fit_transform(test_df['reviewText'])\n",
    "    \n",
    "    #topic identification\n",
    "    topics, reviews = return_topics(vectorizer, clf, transform, test_df, n_top_words, n_top_documents)\n",
    "    # review summarization\n",
    "    summary = summarize_reviews(topics, reviews)\n",
    "    #print(\"Summary :\\n\", summary)\n",
    "    print(\"Topics:\")\n",
    "    \n",
    "    return topics, reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin              reviewerName   helpful  \\\n",
      "0   AO94DHGC771SJ  0528881469                   amazdnu    [0, 0]   \n",
      "1   AMO214LNFCEI4  0528881469           Amazon Customer  [12, 15]   \n",
      "2  A3N7T0DY83Y4IG  0528881469             C. A. Freeman  [43, 45]   \n",
      "3  A1H8PY3QHMQQA0  0528881469  Dave M. Shaw \"mack dave\"   [9, 10]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  We got this GPS for my husband who is an (OTR)...      5.0   \n",
      "1  I'm a professional OTR truck driver, and I bou...      1.0   \n",
      "2  Well, what can I say.  I've had this unit in m...      3.0   \n",
      "3  Not going to write a long review, even thought...      2.0   \n",
      "\n",
      "                   summary  unixReviewTime   reviewTime  \n",
      "0          Gotta have GPS!      1370131200   06 2, 2013  \n",
      "1        Very Disappointed      1290643200  11 25, 2010  \n",
      "2           1st impression      1283990400   09 9, 2010  \n",
      "3  Great grafics, POOR GPS      1290556800  11 24, 2010  \n"
     ]
    }
   ],
   "source": [
    "#reviews_df = getDF('Video_Games_5.json.gz')\n",
    "reviews_df = getDF('Electronics_5.json.gz')\n",
    "print(reviews_df.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy(n):\n",
    "    return -np.log(1/n)\n",
    "\n",
    "def unique(sequence):\n",
    "    '''get unique elements of list and keep the same order'''\n",
    "    \n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "def redundancy(string):\n",
    "    entropy, string_list = 0, string.split()\n",
    "    string_set = unique(string_list)\n",
    "    for word in string_set:\n",
    "        p = string_list.count(word)/len(string_list)\n",
    "        entropy -= p*np.log(p)        \n",
    "    return 1 - entropy/max_entropy(len(string_list))\n",
    "\n",
    "def lemmatize(string):\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = parser(string)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(stemmer.stem(token.lemma_.lower().strip()))\n",
    "        \n",
    "    return ' '.join(lemma for lemma in lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certify midrange bang buck live room home theater altec lansing\n",
      "0.019223019952826492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "['sound music bass', 'z 2300', 'great sound', 'computer speakers buy', 'cable remote satellite', 'card good sound', 'work bose great', 'set speaker best']\n",
      "sound music bass z 2300 great sound computer speakers buy cable remote satellite card good sound work bose great set speaker best\n",
      "0.06885190025657495\n"
     ]
    }
   ],
   "source": [
    "string = 'speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certified midrange bang for the buck living room home theater altec lansing'\n",
    "lemmas = tokenizeText(string) \n",
    "lemmas = ' '.join(lemma for lemma in lemmas)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "\n",
    "\n",
    "\n",
    "topics, review = print_topics('B0002SQ2P2')\n",
    "print(topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earbuds sound quality couple months every 6 months sound good great sound bass volume ears earbud earphones hear inexpensive jack model break cord\n",
      "0.06492466861659307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "['headphones music sound', 'years home studio', 'wear head ears', 'great comfortable', 'hz response flat', 'price expect feel', 'say read review', 'fit amp headphone']\n",
      "headphones music sound years home studio wear head ears great comfortable hz response flat price expect feel say read review fit amp headphone\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "string = 'earbuds sound quality couple months every 6 months sound was good great sound bass volume ears earbud earphones hear inexpensive jack model broke cord'\n",
    "lemmas = tokenizeText(string) \n",
    "lemmas = ' '.join(lemma for lemma in lemmas)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "\n",
    "\n",
    "\n",
    "topics, review = print_topics('B0002D03ZW')\n",
    "print(topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
